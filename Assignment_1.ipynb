{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FECp14-d_F2e"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S197wGuvmEc3"
      },
      "source": [
        "hello\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za-DgcYB_IQx",
        "outputId": "575160cb-cf01-4cff-8d37-8a8908a70c86"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/NLP-Reichman/2025_assignment_1.git\n",
        "# !mv 2025_assignment_1/data data\n",
        "# !rm 2025_assignment_1/ -r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i2bOXTB8Dvc"
      },
      "source": [
        "# Introduction\n",
        "In this assignment you will be creating tools for learning and testing language models. The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
        "The relevant files are under the data folder:\n",
        "\n",
        "- en.csv (or the equivalent JSON file)\n",
        "- es.csv (or the equivalent JSON file)\n",
        "- fr.csv (or the equivalent JSON file)\n",
        "- in.csv (or the equivalent JSON file)\n",
        "- it.csv (or the equivalent JSON file)\n",
        "- nl.csv (or the equivalent JSON file)\n",
        "- pt.csv (or the equivalent JSON file)\n",
        "- tl.csv (or the equivalent JSON file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1u1qR7iaq_GU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "# from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from itertools import product\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHN0tWTurwkN"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56aKA0K8adr"
      },
      "source": [
        "## Part 1\n",
        "Implement the function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. Our token definition is a single UTF-8 encoded character. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data. The vocabulary should include the `<start>` and    `<end>` tokens.\n",
        "\n",
        "Note - do NOT lowecase the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ws_5u7vRrg0o"
      },
      "outputs": [],
      "source": [
        "def preprocess() -> list[str]:\n",
        "\t'''\n",
        "\tReturn a list of characters, representing the shared vocabulary of all languages\n",
        "\t'''\n",
        "\tvocab = []\n",
        "\tdata_files = [file for file in os.listdir('data') if file.endswith('.csv')]\n",
        "\tfor file in data_files:\n",
        "\t\twith open(os.path.join('data', file), 'r') as f:\n",
        "\t\t\tdata = pd.read_csv(f)\n",
        "\t\t\tfor tweet in data['tweet_text']:\n",
        "\t\t\t\tvocab.extend(list(tweet))\n",
        "\t\n",
        "\tvocab.append('<start>')\n",
        "\tvocab.append('<end>')        \n",
        "\tvocab = list(set(vocab))\n",
        "\t\n",
        "\treturn vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1jcvg0jtMjF",
        "outputId": "b05d8228-f84f-40fc-be2f-9be57ff3ead9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab length: 1804\n",
            "Some characters in the vocab: ['Ï©ú', '‚ñΩ', 'Îùº', 'üá±', 'Êò†', '√®', 'üìñ', 'ÎßÅ', '„Ö°', '‚óº']\n"
          ]
        }
      ],
      "source": [
        "vocab = preprocess()\n",
        "print(f\"vocab length: {len(vocab)}\")\n",
        "print(f\"Some characters in the vocab: {vocab[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpjtwHW08jyH"
      },
      "source": [
        "## Part 2\n",
        "Implement the function *build_lm* that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant *n*-1 sequences, and the values are dictionaries with the *n*_th tokens and their corresponding probabilities to occur. To ensure consistent probabilities calculation, please add n-1 `<start>` tokens to the beginning of a tweet and one `<end>` token at the end. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{ \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25}, \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1} }\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it.\n",
        "\n",
        "Please add the `<unk>` token with $p(<unk>)=1|V|$ to the LM if buiulding a smoothed LM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uySEXdEUrkq_"
      },
      "outputs": [],
      "source": [
        "def get_all_tweets(lang: str,number_of_starts: int = 1) -> list[str]:\n",
        "\tdata_files = [file for file in os.listdir('data') if file.endswith('.csv') and file.startswith(lang)]\n",
        "\tall_tweets = []\n",
        "\tfor file in data_files:\n",
        "\t\twith open(os.path.join('data', file), 'r') as f:\n",
        "\t\t\tdata = pd.read_csv(f)\n",
        "\t\t\tfor tweet in data['tweet_text']:\n",
        "\t\t\t\ttext = ['<start>']*number_of_starts + list(tweet) + ['<end>']\n",
        "\t\t\t\tall_tweets.append(text)\n",
        "\treturn all_tweets\n",
        "\n",
        "\n",
        "def get_ngrams(text: str, n: int) -> list[str]:\n",
        "\tngrams = []\n",
        "\tfor i in range(len(text) - n + 1):\n",
        "\t\tngrams.append(text[i:i+n])\n",
        "\treturn ngrams\n",
        "\n",
        "def count_next_tokens(LM: dict[str, dict[str, float]], n: int,all_tweets: list[str]) -> dict[str, dict[str, float]]:\n",
        "\tn_ngrams = (get_ngrams(tweet, n) for tweet in all_tweets)\n",
        "\tfor sentence in n_ngrams:\n",
        "\t\tfor word in sentence:\n",
        "\t\t\tLM[\"\".join(word[:-1])][word[-1]] += 1\n",
        "\treturn LM\n",
        "\n",
        "def add_unknown_tokens(LM: dict[str, dict[str, float]]) -> dict[str, dict[str, float]]:\n",
        "\tfor key in LM:\n",
        "\t\tLM[key]['<unk>'] = 1 \n",
        "\treturn LM\n",
        "\n",
        "\n",
        "def normalize_LM(LM: dict[str, dict[str, float]]) -> dict[str, dict[str, float]]:\n",
        "\tfor key in LM:\n",
        "\t\ttotal = sum(LM[key].values())\n",
        "\t\tfor next_token in LM[key]:\n",
        "\t\t\tif total != 0:\n",
        "\t\t\t\tLM[key][next_token] /= total\n",
        "\treturn LM\n",
        "\n",
        "def build_1_gram_LM(vocab: list[str], smoothed: bool = False, lang: str = 'en') -> dict[str, dict[str, float]]:\n",
        "\tall_chars = []\n",
        "\tdata_files = [file for file in os.listdir('data') if file.endswith('.csv') and file.startswith(lang)]\n",
        "\tfor file in data_files:\n",
        "\t\twith open(os.path.join('data', file), 'r') as f:\n",
        "\t\t\tdata = pd.read_csv(f)\n",
        "\t\t\tfor tweet in data['tweet_text']:\n",
        "\t\t\t\tall_chars.extend(list(tweet))\n",
        "\t\n",
        "\tall_chars.append('<start>')\n",
        "\tall_chars.append('<end>')\n",
        "\tif smoothed:\n",
        "\t\tall_chars.append('<unk>')\n",
        "\t\tvocab.append('<unk>')\n",
        "\tall_chars = list(all_chars)\n",
        "\n",
        " \n",
        "\tvocab_prob = {char: all_chars.count(char)/len(all_chars) for char in vocab}\n",
        "\tLM = {char: vocab_prob.copy() for char in vocab}\n",
        "\treturn LM\n",
        "\n",
        "\n",
        "def build_lm(lang: str, n: int, smoothed: bool = False) -> dict[str, dict[str, float]]:\n",
        "\t'''\n",
        "\tReturn a language model for the given lang and n_gram (n)\n",
        "\t:param lang: the language of the model\n",
        "\t:param n: the n_gram value\n",
        "\t:param smoothed: boolean indicating whether to apply smoothing\n",
        "\t:return: a dictionary where the keys are n_grams and the values are dictionaries\n",
        "\t'''\n",
        "\tLM = {}\n",
        "\tif n == 1:\n",
        "\t\treturn build_1_gram_LM(vocab, smoothed, lang)\n",
        "\tall_tweets = get_all_tweets(lang,n-1)\n",
        "\tkeys_t = (get_ngrams(tweet, n-1) for tweet in all_tweets)\n",
        "\tsentences = (ngram for ngram in keys_t)\n",
        "\tkeys = []\n",
        "\tfor sentence in sentences:\n",
        "\t\tkeys.extend(sentence)\n",
        "\t\n",
        "\t\n",
        "\tvocab_prob = {char: int(smoothed) for char in vocab}\n",
        "\tif smoothed:\n",
        "\t\tkeys.append('<unk>')\n",
        "\tLM = {\"\".join(key): vocab_prob.copy() for key in keys if key[-1] != '<end>'}\n",
        "\tif smoothed:\n",
        "\t\tLM = add_unknown_tokens(LM)\n",
        "\tLM = count_next_tokens(LM, n, all_tweets)\n",
        "\tLM = normalize_LM(LM)\n",
        "\treturn LM\n",
        "\t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_build_lm():\n",
        "\treturn {\n",
        "\t\t'english_2_gram_length': len(build_lm('en', 2, True)),\n",
        "\t\t'english_3_gram_length': len(build_lm('en', 3, True)),\n",
        "\t\t'french_3_gram_length': len(build_lm('fr', 3, True)),\n",
        "\t\t'spanish_3_gram_length': len(build_lm('es', 3, True)),\n",
        "\t}\n",
        "\t\n",
        "def g_test_build_lm(results):\n",
        "\tif results[\"english_2_gram_length\"] != 748:\n",
        "\t\treturn f\"English 2-gram length is {results['english_2_gram_length']}, expected 748\"\n",
        "\tif results[\"english_3_gram_length\"] != 8239:\n",
        "\t\treturn f\"English 3-gram length is {results['english_3_gram_length']}, expected 8239\"\n",
        "\tif results[\"french_3_gram_length\"] != 8286:\n",
        "\t\treturn f\"French 3-gram length is {results['french_3_gram_length']}, expected 8286\"\n",
        "\tif results[\"spanish_3_gram_length\"] != 8469:\n",
        "\t\treturn f\"Spanish 3-gram length is {results['spanish_3_gram_length']}, expected 8469\"\n",
        "\treturn 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9mqbEhBttmm",
        "outputId": "df92d141-999d-42c9-8c12-e6d5e51f7d81"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# LM = build_lm(\"en\", 3, False)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# print(f\"English Language Model with 3-gram is of length: {len(LM)}\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43mtest_build_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(g_test_build_lm(results))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtest_build_lm\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_build_lm\u001b[39m():\n\u001b[32m      2\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \t\t\u001b[33m'\u001b[39m\u001b[33menglish_2_gram_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mbuild_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m),\n\u001b[32m      4\u001b[39m \t\t\u001b[33m'\u001b[39m\u001b[33menglish_3_gram_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(build_lm(\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[32m      5\u001b[39m \t\t\u001b[33m'\u001b[39m\u001b[33mfrench_3_gram_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(build_lm(\u001b[33m'\u001b[39m\u001b[33mfr\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[32m      6\u001b[39m \t\t\u001b[33m'\u001b[39m\u001b[33mspanish_3_gram_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(build_lm(\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[32m      7\u001b[39m \t}\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mbuild_lm\u001b[39m\u001b[34m(lang, n, smoothed)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m smoothed:\n\u001b[32m     61\u001b[39m \tkeys.append(\u001b[33m'\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m LM = {\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(key): vocab_prob.copy() \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m key[-\u001b[32m1\u001b[39m] != \u001b[33m'\u001b[39m\u001b[33m<end>\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m smoothed:\n\u001b[32m     64\u001b[39m \tLM = add_unknown_tokens(LM)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# LM = build_lm(\"en\", 3, False)\n",
        "# print(f\"English Language Model with 3-gram is of length: {len(LM)}\")\n",
        "results = test_build_lm()\n",
        "print(g_test_build_lm(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZnk7Ke8rW5"
      },
      "source": [
        "## Part 3\n",
        "Implement the function *eval* that returns the perplexity of a model (dictionary) running over the data file of the given target language.\n",
        "\n",
        "The `<unk>` should be used for unknown contexts when calculating the perplexities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y9w8u411uJeq"
      },
      "outputs": [],
      "source": [
        "def perplexity(model: dict, text: list, n:int) -> float:\n",
        "\t'''\n",
        "\tCalculates the perplexity of the given string using the given language model.\n",
        "\t:param model: The language model\n",
        "\t:param text: The tokenized text to calculate the perplexity for\n",
        "\t:param n: The n-gram of the model\n",
        "\t:return: The perplexity\n",
        "\t'''\n",
        "\tpp = 0\n",
        "\tlog_prob_sum = 0.0\t\n",
        "\tcount = 0\n",
        "\n",
        "\tkeys_t = (get_ngrams(tweet, n) for tweet in text)\n",
        "\tsentences = (ngram for ngram in keys_t)\n",
        "\tkeys = []\n",
        "\tfor sentence in sentences:\n",
        "\t\tkeys.extend(sentence)\n",
        "\tfor ngram in keys:\n",
        "\t\tcontext = ''.join(ngram[:-1])\n",
        "\t\tnext_char = ngram[-1]\t\n",
        "\t\tcontext_probs = model.get(context, model['<unk>'])\n",
        "\t\tprob = context_probs.get(next_char, context_probs['<unk>'])\n",
        "\t\tlog_prob_sum += math.log(prob)\n",
        "\t\tcount += 1\t\n",
        "\tif count == 0:\n",
        "\t\treturn float('inf')\n",
        "\treturn math.exp(-log_prob_sum / count)\t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ef-EglxXrmk2"
      },
      "outputs": [],
      "source": [
        "def eval(model: dict, target_lang: str, n: int) -> float:\n",
        "\t'''\n",
        "\tReturn the perplexity value calculated over applying the model on the text file\n",
        "\tof the target_lang language.\n",
        "\t:param model: the language model\n",
        "\t:param target_lang: the target language\n",
        "\t:param n: The n-gram of the model\n",
        "\t:return: the perplexity value\n",
        "\t'''\n",
        "\tpp = 0\n",
        "\n",
        "\tall_tweets = get_all_tweets(target_lang, n)\n",
        "\tpp = perplexity(model, all_tweets, n)\n",
        "\treturn pp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AIdDFvinBVhx"
      },
      "outputs": [],
      "source": [
        "LM = build_lm(\"en\", 3, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WUouhkBuWJu",
        "outputId": "75967c86-5b00-480a-8ffd-574122f7a452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity of the English 3-gram model on datasets:\n",
            "On English:  28.47\n",
            "On French:  61.67\n",
            "On Dutch:  64.91\n",
            "On Tagalog:  75.36\n"
          ]
        }
      ],
      "source": [
        "print(\"Perplexity of the English 3-gram model on datasets:\")\n",
        "print(f\"On English: {eval(LM, 'en', 3): .2f}\")\n",
        "print(f\"On French: {eval(LM, 'fr', 3): .2f}\")\n",
        "print(f\"On Dutch: {eval(LM, 'nl', 3): .2f}\")\n",
        "print(f\"On Tagalog: {eval(LM, 'tl', 3): .2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XP3ZIpLqB6r",
        "outputId": "3378ada3-42c1-42fb-88eb-8eeec458d0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity on differnet n-gram models on English\n",
            "On 1-gram:  45.65\n",
            "On 2-gram:  22.73\n",
            "On 3-gram:  28.47\n",
            "On 4-gram:  63.23\n"
          ]
        }
      ],
      "source": [
        "lm1 = build_lm(\"en\", 1, True)\n",
        "lm2 = build_lm(\"en\", 2, True)\n",
        "lm3 = build_lm(\"en\", 3, True)\n",
        "lm4 = build_lm(\"en\", 4, True)\n",
        "\n",
        "print(\"Perplexity on differnet n-gram models on English\")\n",
        "print(f\"On 1-gram: {eval(lm1, 'en', 1): .2f}\")\n",
        "print(f\"On 2-gram: {eval(lm2, 'en', 2): .2f}\")\n",
        "print(f\"On 3-gram: {eval(lm3, 'en', 3): .2f}\")\n",
        "print(f\"On 4-gram: {eval(lm4, 'en', 4): .2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZYVc7hB84LP"
      },
      "source": [
        "## Part 4\n",
        "Implement the *match* function that calls *eval* using a specific value of *n* for every possible language pair among the languages we have data for. You should call *eval* for every language pair four times, with each call assign a different value for *n* (1-4). Each language pair is composed of the source language and the target language. Before you make the call, you need to call the *lm* function to create the language model for the source language. Then you can call *eval* with the language model and the target language. The function should return a pandas DataFrame with the following four columns: *source_lang*, *target_lang*, *n*, *perplexity*. The values for the first two columns are the two-letter language codes. The value for *n* is the *n* you use for generating the specific perplexity values which you should store in the forth column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tMczigsHuadi"
      },
      "outputs": [],
      "source": [
        "languages = ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "16ew9aZWroPC"
      },
      "outputs": [],
      "source": [
        "def match() -> pd.DataFrame:\n",
        "\t'''\n",
        "\tReturn a DataFrame containing one line per every language pair and n_gram.\n",
        "\tEach line will contain the perplexity calculated when applying the language model\n",
        "\tof the source language on the text of the target language.\n",
        "\t:return: a DataFrame containing the perplexity values\n",
        "\t'''\n",
        "\tdf    = pd.DataFrame()\n",
        "\tfor lang in languages:\n",
        "\t\tfor target_lang in languages:\n",
        "\t\t\tif lang != target_lang:\n",
        "\t\t\t\tfor n in range(1, 5):\n",
        "\t\t\t\t\tlm = build_lm(lang, n, True)\n",
        "\t\t\t\t\tperplexity = eval(lm, target_lang, n)\n",
        "\t\t\t\t\tdf = pd.concat([df, pd.DataFrame({'source': lang, 'target': target_lang, 'n': n, 'perplexity': perplexity}, index=[0])], ignore_index=True)\n",
        "\treturn df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "math domain error",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected increasing perplexity from English to other languages. Results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mtest_match_grader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest_match_grader\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_match_grader\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     results = \u001b[43mtest_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     perplexity_en_on_en = \u001b[38;5;28mint\u001b[39m(results[\u001b[33m\"\u001b[39m\u001b[33men_en_3\u001b[39m\u001b[33m\"\u001b[39m])  \n\u001b[32m     13\u001b[39m     perplexity_en_on_tl = \u001b[38;5;28mint\u001b[39m(results[\u001b[33m\"\u001b[39m\u001b[33men_tl_3\u001b[39m\u001b[33m\"\u001b[39m])  \n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtest_match\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_match\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df = \u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     df.to_csv(\u001b[33m'\u001b[39m\u001b[33mmatch.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      5\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdf_shape\u001b[39m\u001b[33m'\u001b[39m: df.shape,\n\u001b[32m      6\u001b[39m         \u001b[33m'\u001b[39m\u001b[33men_en_3\u001b[39m\u001b[33m'\u001b[39m: df[(df[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m3\u001b[39m)][\u001b[33m'\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m'\u001b[39m].values[\u001b[32m0\u001b[39m],\n\u001b[32m      7\u001b[39m         \u001b[33m'\u001b[39m\u001b[33men_tl_3\u001b[39m\u001b[33m'\u001b[39m: df[(df[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtl\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m3\u001b[39m)][\u001b[33m'\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m'\u001b[39m].values[\u001b[32m0\u001b[39m],\n\u001b[32m      8\u001b[39m         \u001b[33m'\u001b[39m\u001b[33men_nl_3\u001b[39m\u001b[33m'\u001b[39m: df[(df[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mnl\u001b[39m\u001b[33m'\u001b[39m) & (df[\u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m3\u001b[39m)][\u001b[33m'\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m'\u001b[39m].values[\u001b[32m0\u001b[39m],\n\u001b[32m      9\u001b[39m     }\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mmatch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m \t\t\t\u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m     13\u001b[39m \t\t\t\tlm = build_lm(lang, n, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \t\t\t\tperplexity = \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \t\t\t\tdf = pd.concat([df, pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: lang, \u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m: target_lang, \u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m: n, \u001b[33m'\u001b[39m\u001b[33mperplexity\u001b[39m\u001b[33m'\u001b[39m: perplexity}, index=[\u001b[32m0\u001b[39m])], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36meval\u001b[39m\u001b[34m(model, target_lang, n)\u001b[39m\n\u001b[32m     10\u001b[39m pp = \u001b[32m0\u001b[39m\n\u001b[32m     12\u001b[39m all_tweets = get_all_tweets(target_lang, n)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m pp = \u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_tweets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pp\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mperplexity\u001b[39m\u001b[34m(model, text, n)\u001b[39m\n\u001b[32m     21\u001b[39m \tcontext_probs = model.get(context, model[\u001b[33m'\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     22\u001b[39m \tprob = context_probs.get(next_char, context_probs[\u001b[33m'\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \tlog_prob_sum += \u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \tcount += \u001b[32m1\u001b[39m\t\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count == \u001b[32m0\u001b[39m:\n",
            "\u001b[31mValueError\u001b[39m: math domain error"
          ]
        }
      ],
      "source": [
        "def test_match():\n",
        "    df = match()\n",
        "    df.to_csv('match.csv', index=False)\n",
        "    return {\n",
        "        'df_shape': df.shape,\n",
        "        'en_en_3': df[(df['source'] == 'en') & (df['target'] == 'en') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "        'en_tl_3': df[(df['source'] == 'en') & (df['target'] == 'tl') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "        'en_nl_3': df[(df['source'] == 'en') & (df['target'] == 'nl') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "    }\n",
        "def test_match_grader():\n",
        "    results = test_match()\n",
        "    perplexity_en_on_en = int(results[\"en_en_3\"])  \n",
        "    perplexity_en_on_tl = int(results[\"en_tl_3\"])  \n",
        "    perplexity_en_on_nl = int(results[\"en_nl_3\"])  \n",
        "\n",
        "    perplexities = [\n",
        "        perplexity_en_on_en,\n",
        "        perplexity_en_on_tl,\n",
        "        perplexity_en_on_nl\n",
        "    ]\n",
        "\n",
        "    if min(perplexities) != perplexity_en_on_en:\n",
        "        return f\"English model should perform best on English text. Results: {results}\"\n",
        "\n",
        "    if not (perplexity_en_on_en <= max(perplexity_en_on_tl, perplexity_en_on_nl)):\n",
        "        return f\"Expected increasing perplexity from English to other languages. Results: {results}\"\n",
        "\n",
        "    return 1\n",
        "\n",
        "test_match_grader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAQoR0dH9C3T"
      },
      "source": [
        "## Part 5\n",
        "Implement the *generate* function which takes a language code, *n*, the prompt (the starting text), the number of tokens to generate, and *r*, which is the random seed for any randomized action you plan to take in your implementation. The function should start generating tokens, one by one, using the language model of the given source language and *n*. The prompt should be used as a starting point for aligning on the probabilities to be used for generating the next token.\n",
        "\n",
        "Note - The generation of the next token should be from the LM's distribution with NO smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CpCm24-RrpuA"
      },
      "outputs": [],
      "source": [
        "def generate(lang: str, n: int, prompt: str, number_of_tokens: int, r: int) -> str:\n",
        "\t'''\n",
        "\tGenerate text in the given language using the given parameters.\n",
        "\t:param lang: the language of the model\n",
        "\t:param n: the n_gram value\n",
        "\t:param prompt: the prompt to start the generation\n",
        "\t:param number_of_tokens: the number of tokens to generate\n",
        "\t:param r: the random seed to use\n",
        "\t'''\n",
        "\n",
        "\tlm = build_lm(lang, n, False)\n",
        "\trandom.seed(r)\n",
        "\tfor i in range(number_of_tokens):\n",
        "\t\tif n == 1:\n",
        "\t\t\tprev_n_gram = prompt[-1]\n",
        "\t\t\tnext_token = np.random.choice(list(lm[prev_n_gram].keys()),p=list(lm[prev_n_gram].values()))\n",
        "\t\telse:\n",
        "\t\t\tprev_n_gram = prompt[-n+1:]\n",
        "\t\t\tnext_token = np.random.choice(list(lm[prev_n_gram].keys()),p=list(lm[prev_n_gram].values()))\n",
        "\t\tif next_token == '<unk>':\n",
        "\t\t\tnext_token = random.choice(list(lm['<unk>'].keys()))\n",
        "\t\tif next_token == '<end>':\n",
        "\t\t\tbreak\n",
        "\t\tprompt += next_token\n",
        "\t\n",
        "\treturn prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWX8Ugu9INH"
      },
      "source": [
        "## Part 6\n",
        "Play with your generate function, try to generate different texts in different language and various values of *n*. No need to submit anything of that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ykbMBzG9LWn",
        "outputId": "c9613bb9-9d55-48dd-d2bf-f79435ab6d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I amvOli n.i p\n",
            "I ampsorextter\n",
            "I amanaldTrung\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(generate(\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mI am\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(generate(\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mI am\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI am \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(generate(\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSoy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(generate(\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSoy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(lang, n, prompt, number_of_tokens, r)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(lang: \u001b[38;5;28mstr\u001b[39m, n: \u001b[38;5;28mint\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, number_of_tokens: \u001b[38;5;28mint\u001b[39m, r: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\t\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\tGenerate text in the given language using the given parameters.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\t:param lang: the language of the model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m\t:param r: the random seed to use\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\t'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \tlm = \u001b[43mbuild_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \trandom.seed(r)\n\u001b[32m     13\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_tokens):\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mbuild_lm\u001b[39m\u001b[34m(lang, n, smoothed)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m smoothed:\n\u001b[32m     86\u001b[39m \tLM = add_unknown_tokens(LM)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m LM = \u001b[43mcount_next_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_tweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m LM = normalize_LM(LM)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LM\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcount_next_tokens\u001b[39m\u001b[34m(LM, n, all_tweets)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m n_ngrams:\n\u001b[32m     22\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \t\tLM[\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(word[:-\u001b[32m1\u001b[39m])][word[-\u001b[32m1\u001b[39m]] += \u001b[32m1\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LM\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(generate('en', 1, \"I am\", 10, 5))\n",
        "print(generate('en', 2, \"I am\", 10, 5))\n",
        "print(generate('en', 3, \"I am\", 10, 5))\n",
        "print(generate('en', 4, \"I am \", 10, 5))\n",
        "print(generate('es', 2, \"Soy\", 10, 5))\n",
        "print(generate('es', 3, \"Soy\", 10, 5))\n",
        "print(generate('fr', 2, \"Je suis\", 10, 5))\n",
        "print(generate('fr', 3, \"Je suis\", 10, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2jNlDISr9aL"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv48OCT_sIYW"
      },
      "source": [
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JZTlc2ieruqq",
        "outputId": "772800de-c13a-4bd2-f22e-734b012da84f"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# PLACE TESTS HERE #\n",
        "# Create tests\n",
        "def test_preprocess():\n",
        "\treturn {\n",
        "\t\t'vocab_length': len(preprocess()),\n",
        "\t}\n",
        "\n",
        "def test_build_lm():\n",
        "\treturn {\n",
        "\t\t'english_2_gram_length': len(build_lm('en', 2, True)),\n",
        "\t\t'english_3_gram_length': len(build_lm('en', 3, True)),\n",
        "\t\t'french_3_gram_length': len(build_lm('fr', 3, True)),\n",
        "\t\t'spanish_3_gram_length': len(build_lm('es', 3, True)),\n",
        "\t}\n",
        "\n",
        "def test_eval():\n",
        "\tlm = build_lm('en', 3, True)\n",
        "\treturn {\n",
        "\t\t'en_on_en': round(eval(lm, 'en', 3), 2),\n",
        "\t\t'en_on_fr': round(eval(lm, 'fr', 3), 2),\n",
        "\t\t'en_on_tl': round(eval(lm, 'tl', 3), 2),\n",
        "\t\t'en_on_nl': round(eval(lm, 'nl', 3), 2),\n",
        "\t}\n",
        "\n",
        "def test_match():\n",
        "\tdf = match()\n",
        "\treturn {\n",
        "\t\t'df_shape': df.shape,\n",
        "\t\t'en_en_3': df[(df['source'] == 'en') & (df['target'] == 'en') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "\t\t'en_tl_3': df[(df['source'] == 'en') & (df['target'] == 'tl') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "\t\t'en_nl_3': df[(df['source'] == 'en') & (df['target'] == 'nl') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "\t}\n",
        "\n",
        "def test_generate():\n",
        "\treturn {\n",
        "\t\t'english_2_gram': generate('en', 2, \"I am\", 20, 5),\n",
        "\t\t'english_3_gram': generate('en', 3, \"I am\", 20, 5),\n",
        "\t\t'english_4_gram': generate('en', 4, \"I Love\", 20, 5),\n",
        "\t\t'spanish_2_gram': generate('es', 2, \"Soy\", 20, 5),\n",
        "\t\t'spanish_3_gram': generate('es', 3, \"Soy\", 20, 5),\n",
        "\t\t'french_2_gram': generate('fr', 2, \"Je suis\", 20, 5),\n",
        "\t\t'french_3_gram': generate('fr', 3, \"Je suis\", 20, 5),\n",
        "\t}\n",
        "\n",
        "TESTS = [test_preprocess, test_build_lm, test_eval, test_match, test_generate]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "\ttry:\n",
        "\t\tcur_res = test()\n",
        "\t\tres.update({test.__name__: cur_res})\n",
        "\texcept Exception as e:\n",
        "\t\tres.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "\tjson.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "# files.download('results.json')\n",
        "########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dEpidyrqXTr",
        "outputId": "771c371b-d07c-4aee-fd4e-8bca0a9d31f3"
      },
      "outputs": [],
      "source": [
        "# Show the local files, results.json should be there now and\n",
        "# also downloaded to your local machine\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA8l8Vg5hPtr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
